{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: youtube-transcript-api in c:\\python310\\lib\\site-packages (0.6.1)\n",
      "Requirement already satisfied: requests in c:\\python310\\lib\\site-packages (from youtube-transcript-api) (2.31.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python310\\lib\\site-packages (from requests->youtube-transcript-api) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python310\\lib\\site-packages (from requests->youtube-transcript-api) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python310\\lib\\site-packages (from requests->youtube-transcript-api) (2023.7.22)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\python310\\lib\\site-packages (from requests->youtube-transcript-api) (3.2.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 23.2.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in c:\\python310\\lib\\site-packages (0.27.8)\n",
      "Requirement already satisfied: requests>=2.20 in c:\\python310\\lib\\site-packages (from openai) (2.31.0)\n",
      "Requirement already satisfied: aiohttp in c:\\python310\\lib\\site-packages (from openai) (3.8.5)\n",
      "Requirement already satisfied: tqdm in c:\\python310\\lib\\site-packages (from openai) (4.65.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python310\\lib\\site-packages (from requests>=2.20->openai) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\python310\\lib\\site-packages (from requests>=2.20->openai) (3.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python310\\lib\\site-packages (from requests>=2.20->openai) (2023.7.22)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python310\\lib\\site-packages (from requests>=2.20->openai) (2.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\python310\\lib\\site-packages (from aiohttp->openai) (1.9.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\python310\\lib\\site-packages (from aiohttp->openai) (6.0.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\python310\\lib\\site-packages (from aiohttp->openai) (23.1.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\python310\\lib\\site-packages (from aiohttp->openai) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\python310\\lib\\site-packages (from aiohttp->openai) (1.4.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\python310\\lib\\site-packages (from aiohttp->openai) (4.0.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\vish\\appdata\\roaming\\python\\python310\\site-packages (from tqdm->openai) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 23.2.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\python310\\lib\\site-packages (2.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\vish\\appdata\\roaming\\python\\python310\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\python310\\lib\\site-packages (from pandas) (1.25.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\python310\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\python310\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\vish\\appdata\\roaming\\python\\python310\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 23.2.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in c:\\python310\\lib\\site-packages (0.0.247)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\python310\\lib\\site-packages (from langchain) (8.2.2)\n",
      "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in c:\\python310\\lib\\site-packages (from langchain) (2.8.4)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\python310\\lib\\site-packages (from langchain) (2.0.19)\n",
      "Requirement already satisfied: pydantic<2,>=1 in c:\\python310\\lib\\site-packages (from langchain) (1.10.12)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\python310\\lib\\site-packages (from langchain) (3.8.5)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\python310\\lib\\site-packages (from langchain) (1.25.1)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in c:\\python310\\lib\\site-packages (from langchain) (4.0.2)\n",
      "Requirement already satisfied: langsmith<0.1.0,>=0.0.11 in c:\\python310\\lib\\site-packages (from langchain) (0.0.15)\n",
      "Requirement already satisfied: openapi-schema-pydantic<2.0,>=1.2 in c:\\python310\\lib\\site-packages (from langchain) (1.2.4)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\python310\\lib\\site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in c:\\python310\\lib\\site-packages (from langchain) (0.5.13)\n",
      "Requirement already satisfied: PyYAML>=5.4.1 in c:\\python310\\lib\\site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\python310\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\python310\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\python310\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\python310\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\python310\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\python310\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\python310\\lib\\site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\python310\\lib\\site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (3.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\python310\\lib\\site-packages (from pydantic<2,>=1->langchain) (4.7.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python310\\lib\\site-packages (from requests<3,>=2->langchain) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python310\\lib\\site-packages (from requests<3,>=2->langchain) (2023.7.22)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python310\\lib\\site-packages (from requests<3,>=2->langchain) (3.4)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\python310\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.2)\n",
      "Requirement already satisfied: packaging>=17.0 in c:\\users\\vish\\appdata\\roaming\\python\\python310\\site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (23.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\python310\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (1.0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 23.2.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pinecone-client in c:\\python310\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: pyyaml>=5.4 in c:\\python310\\lib\\site-packages (from pinecone-client) (6.0.1)\n",
      "Requirement already satisfied: dnspython>=2.0.0 in c:\\python310\\lib\\site-packages (from pinecone-client) (2.4.2)\n",
      "Requirement already satisfied: urllib3>=1.21.1 in c:\\python310\\lib\\site-packages (from pinecone-client) (2.0.4)\n",
      "Requirement already satisfied: numpy>=1.22.0 in c:\\python310\\lib\\site-packages (from pinecone-client) (1.25.1)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\vish\\appdata\\roaming\\python\\python310\\site-packages (from pinecone-client) (2.8.2)\n",
      "Requirement already satisfied: loguru>=0.5.0 in c:\\python310\\lib\\site-packages (from pinecone-client) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.64.1 in c:\\python310\\lib\\site-packages (from pinecone-client) (4.65.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in c:\\python310\\lib\\site-packages (from pinecone-client) (4.7.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\python310\\lib\\site-packages (from pinecone-client) (2.31.0)\n",
      "Requirement already satisfied: colorama>=0.3.4 in c:\\users\\vish\\appdata\\roaming\\python\\python310\\site-packages (from loguru>=0.5.0->pinecone-client) (0.4.6)\n",
      "Requirement already satisfied: win32-setctime>=1.0.0 in c:\\python310\\lib\\site-packages (from loguru>=0.5.0->pinecone-client) (1.1.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\vish\\appdata\\roaming\\python\\python310\\site-packages (from python-dateutil>=2.5.3->pinecone-client) (1.16.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python310\\lib\\site-packages (from requests>=2.19.0->pinecone-client) (2023.7.22)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\python310\\lib\\site-packages (from requests>=2.19.0->pinecone-client) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python310\\lib\\site-packages (from requests>=2.19.0->pinecone-client) (3.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 23.2.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\python310\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: joblib in c:\\python310\\lib\\site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: click in c:\\python310\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: tqdm in c:\\python310\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\python310\\lib\\site-packages (from nltk) (2023.8.8)\n",
      "Requirement already satisfied: colorama in c:\\users\\vish\\appdata\\roaming\\python\\python310\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 23.2.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in c:\\python310\\lib\\site-packages (0.5.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\python310\\lib\\site-packages (from tiktoken) (2023.8.8)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\python310\\lib\\site-packages (from tiktoken) (2.31.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python310\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python310\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\python310\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python310\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2023.7.22)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 23.2.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\python310\\lib\\site-packages (1.3.1)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in c:\\python310\\lib\\site-packages (from scikit-learn) (1.25.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\python310\\lib\\site-packages (from scikit-learn) (3.2.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\python310\\lib\\site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\python310\\lib\\site-packages (from scikit-learn) (1.11.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 23.2.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# make necessary imports of the libraries\n",
    "!pip install youtube-transcript-api\n",
    "!pip install openai\n",
    "!pip install pandas\n",
    "!pip install langchain\n",
    "!pip install pinecone-client\n",
    "!pip install nltk\n",
    "!pip install tiktoken\n",
    "!pip install scikit-learn \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "# Replace 'YOUR_OPENAI_API_KEY' with your actual API key\n",
    "openai.api_key = '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'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_section =\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gettranscriptfromvideo(video_id, filename=\"\"):\n",
    "\n",
    "    # Get the transcript for a video\n",
    "    transcripttext = YouTubeTranscriptApi.get_transcript(video_id)\n",
    "\n",
    "    #format the transcript got from the youtube video\n",
    "    formatter = TextFormatter()\n",
    "    transcript = formatter.format_transcript(transcripttext)\n",
    "    if filename:\n",
    "        #write the transcript to a file\n",
    "        with open(filename + '.txt', 'w') as f:\n",
    "            f.write(transcript)\n",
    "        #find length of the transcript\n",
    "        length = len(transcript)\n",
    "        #if length is zero then print no transcript found\n",
    "        if length == 0:\n",
    "            print(\"No transcript found\")\n",
    "        else:\n",
    "            print(\"Transcript found and written to file\")\n",
    "    return transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text, model=\"text-embedding-ada-002\"):\n",
    "   text = text.replace(\"\\n\", \" \")\n",
    "   return openai.Embedding.create(input = [text], model=model)['data'][0]['embedding']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 26)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m<tokenize>:26\u001b[1;36m\u001b[0m\n\u001b[1;33m    answertext = llm(prompt_text )\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "def get_summary(chunks):\n",
    "\n",
    "    llm = OpenAI(temperature=0.4,openai_api_key=\"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\")\n",
    "    summary_chunks=[]    \n",
    "    template = \"\"\"\n",
    "    Summarise content in the context section. Use content in the previous section for continuity. Please dont state it as summary. Highlight important points. Keep the narration coversational\"\n",
    "\n",
    "    Context section:\n",
    "    {content}\n",
    "\n",
    "    \n",
    "    previous section:\n",
    "    {prev_section}\n",
    "\n",
    "   \"\"\"\n",
    "    prompt = PromptTemplate(template=template, input_variables=[\"content\", \"prev_section\"])\n",
    "    #prompt = PromptTemplate(template=template, input_variables=[\"content\"])\n",
    "\n",
    "    prev_section =\"\"\n",
    "    for text in chunks:\n",
    "        if (len(text) > 5):\n",
    "             prompt_text = prompt.format(content = text, prev_section = prev_section)\n",
    "            #prompt_text = prompt.format(content = text)\n",
    "            # prev_section = llm(prompt_text ) \n",
    "            # print (prev_section)\n",
    "            answertext = llm(prompt_text )\n",
    "            prev_section = answertext\n",
    "            summary_chunks.append(llm(answertext ))\n",
    "    return(summary_chunks)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Vish\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "def split_text (input_text):\n",
    "  #split_texts = sent_tokenize(input_text)\n",
    "  #The sent_tokenize function  tokenize inserted text into sentences\n",
    "  split_texts = sent_tokenize(input_text)\n",
    "  return split_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getembeddingsandsavetovectordb(chunks,video_id, dbindex,vectordbkey ):\n",
    "    text_chunks=[]\n",
    "\n",
    "    for text in chunks:\n",
    "        text_chunks.append(text)\n",
    "\n",
    "    df = pd.DataFrame({'text_chunks': text_chunks})\n",
    "\n",
    "    #loop through the chunks and call function get_embedding to get embeddings for each chunk\n",
    "    df['ada_embedding'] = df.text_chunks.apply(lambda x: get_embedding(x, model='text-embedding-ada-002'))\n",
    "\n",
    "    #store the embeddings in pinecone\n",
    "    pinecone.init(\n",
    "        api_key=vectordbkey,\n",
    "        environment=\"us-west1-gcp-free\")\n",
    "\n",
    "    pindex = pinecone.Index(dbindex)\n",
    "\n",
    "    meta= [{'combined': line} for line in df['text_chunks']]\n",
    "    vectors = list(zip(video_id, df['ada_embedding'], meta))\n",
    "\n",
    "    upsert_response = pindex.upsert(\n",
    "        vectors=vectors,\n",
    "        namespace=dbindex, values=True, include_metadata=True)\n",
    "    return pindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def savetranscriptvectorfromvideo():\n",
    "    video_id = input(\"Enter the video id: \")\n",
    "    chunks = split_transcript_into_chunks (gettranscriptfromvideo(video_id),1000)\n",
    "    print (len(chunks))\n",
    "    #get_summary(chunks)\n",
    "    #vectordbindex = getembeddingsandsavetovectordb(chunks,video_id, \"myyoutubeindex\",\"df750bd6-bfd5-4298-862c-0f9cdfaee6bc\" )\n",
    "    return vectordbindex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "from youtube_transcript_api.formatters import TextFormatter\n",
    "from langchain import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "import requests\n",
    "import pinecone\n",
    "\n",
    "video_id = input(\"Enter the video id: \")\n",
    "chunks = split_transcript_into_chunks (gettranscriptfromvideo(video_id),500)\n",
    "print (\"Chunk length\")\n",
    "print (\"getting summary...\")\n",
    "df = pd.DataFrame({'summary_chunks': get_summary(chunks)})\n",
    "print (\"storing in vectordb..\")\n",
    "vectordbindex = getembeddingsandsavetovectordb(chunks,video_id, \"myyoutubeindex\",\"df750bd6-bfd5-4298-862c-0f9cdfaee6bc\" )\n",
    "answertext = returnresultforquestion(vectordbindex)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def returnresultforquestion(vectordbindex):\n",
    "    questiontext = input(\"Enter your question: \")\n",
    "    question_embedding = get_embedding(text=questiontext, model=\"text-embedding-ada-002\")\n",
    "    content = \"\"\n",
    "    content = content\n",
    "    print (\"first\" + content)\n",
    "    vector_database_results_matching = vectordbindex.query([question_embedding], top_k=3, include_metadata=True, include_Values=True, \n",
    "        namespace=\"myyoutubeindex\")\n",
    "    for match in vector_database_results_matching['matches']:\n",
    "        if float(match['score']) *100 > 75 :\n",
    "            content = content + \" \" + match['metadata']['combined']\n",
    "    if content == \"\":\n",
    "        return \"Sorry, I could not find an answer to your question\"\n",
    "    \n",
    "    print (\"second\" + content)\n",
    "\n",
    "    # define the LLM you want to use\n",
    "    llm = OpenAI(temperature=0.4,openai_api_key=\"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\")\n",
    "\n",
    "    # define the context for the prompt by joining the most relevant text chunks\n",
    "    # context = \"\"\n",
    "\n",
    "    # for index, row in df[0:5].iterrows():\n",
    "    #     context = context + \" \" + row.text_chunks\n",
    "\n",
    "    # define the prompt template\n",
    "    template = \"\"\"\n",
    "    Answer the question in the question section stricty using only the contents from the context section only. If the context section is empty or If you cant find the answer, please respond that way\"\n",
    "\n",
    "    Context section:\n",
    "    {content}\n",
    "\n",
    "    Question section:\n",
    "    {users_question}\n",
    "\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = PromptTemplate(template=template, input_variables=[\"content\", \"users_question\"])\n",
    "\n",
    "    # fill the prompt template\n",
    "    prompt_text = prompt.format(content = content, users_question = questiontext)\n",
    "    answertext = llm(prompt_text )\n",
    "    return answertext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "def getcontextcontent()# create a list to store the calculated cosine similarity\n",
    "    # cos_sim = []\n",
    "\n",
    "    # for index, row in df.iterrows():\n",
    "    #    A = row.ada_embedding\n",
    "    #    B = question_embedding\n",
    "\n",
    "    #    # calculate the cosine similiarity\n",
    "    #    cosine = np.dot(A,B)/(norm(A)*norm(B))\n",
    "\n",
    "    #    cos_sim.append(cosine)\n",
    "\n",
    "    # df[\"cos_sim\"] = cos_sim\n",
    "    # df.sort_values(by=[\"cos_sim\"], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split transcript into chunks of size 2000\n",
    "def split_transcript_into_chunks(transcript, chunk_size):\n",
    "    chunks =[]\n",
    "    chunks = [transcript[i:i + chunk_size] for i in range(0, len(transcript), chunk_size)]\n",
    "    return chunks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text, model=\"text-embedding-ada-002\"):\n",
    "   text = text.replace(\"\\n\", \" \")\n",
    "   return openai.Embedding.create(input = [text], model=model)['data'][0]['embedding']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute the length, split into chunks\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "from youtube_transcript_api.formatters import TextFormatter\n",
    "from langchain import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "import requests\n",
    "import pinecone\n",
    "import tiktoken\n",
    "enc = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "\n",
    "video_id = input(\"Enter the video id: \")\n",
    "videoTranscript = gettranscriptfromvideo(video_id)\n",
    "largeChunks = split_transcript_into_chunks(videoTranscript, 3000)\n",
    "responseText =\"\"\n",
    "\n",
    "#create proper pronounciations and sentences\n",
    "llm = OpenAI(temperature=0.4,openai_api_key=\"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\")\n",
    "for textchunk in largeChunks:\n",
    "    template = \"\"\"\n",
    "    Given a body of text in Transcript, you will output the same text with full stops and punctuations at appropriate places. You will not change the words in the text\"\n",
    "\n",
    "    Transcript:\n",
    "    {content}\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = PromptTemplate(template=template, input_variables=[\"content\"])\n",
    "\n",
    "    # fill the prompt template\n",
    "    prompt_text = prompt.format(content = textchunk)\n",
    "    answertext = llm(prompt_text )\n",
    "    responseText = responseText +  answertext\n",
    "split_sents = split_text(responseText)\n",
    "inputChunks = create_chunks(split_sents, max_token_len=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate embeddings for chunks\n",
    "Transcriptdf = pd.DataFrame({'Textsegments': inputChunks})\n",
    "\n",
    "#loop through the chunks and call function get_embedding to get embeddings for each chunk\n",
    "Transcriptdf['segmentembedding'] = Transcriptdf.Textsegments.apply(lambda x: get_embedding(x, model='text-embedding-ada-002'))\n",
    "\n",
    "    #store the embeddings in pinecone\n",
    "pinecone.init(\n",
    "    api_key=\"df750bd6-bfd5-4298-862c-0f9cdfaee6bc\",\n",
    "    environment=\"us-west1-gcp-free\")\n",
    "\n",
    "pindex = pinecone.Index(\"youtubeindex\")\n",
    "\n",
    "meta= [{'combined': line} for line in Transcriptdf['Textsegments']]\n",
    "vectors = list(zip(video_id, Transcriptdf['segmentembedding'], meta))\n",
    "\n",
    "upsert_response = pindex.upsert(\n",
    "    vectors=vectors,\n",
    "    namespace=\"youtubeindex\", values=True, include_metadata=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def create_chunks(split_sents, max_token_len=2000):\n",
    "  current_token_len = 0\n",
    "  input_chunks = []\n",
    "  current_chunk = \"\"\n",
    "  for sents in split_sents:\n",
    "    sent_token_len = len(enc.encode(sents))\n",
    "    if (current_token_len + sent_token_len) > max_token_len:\n",
    "      input_chunks.append(current_chunk)\n",
    "      current_chunk = \"\"\n",
    "      current_token_len = 0\n",
    "    current_chunk = current_chunk + sents\n",
    "    current_token_len = current_token_len + sent_token_len\n",
    "  if current_chunk != \"\":\n",
    "    input_chunks.append(current_chunk)\n",
    "  return input_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\"What is the nature of YouTube's recommendation system and why does it optimize watch time?\"\n",
      "What is the attack on the YouTube algorithm mentioned in the video and how does it affect the algorithm?\n",
      "\"What is the intuition behind collaborative filtering and how is it used to make predictions for newer users and items?\"\n",
      "\n",
      "What is the goal of computing the similarities between the user vectors and User D?\n",
      "\"What are the pros and cons of user-based collaborative filtering and how does item-based collaborative filtering compare? How can we use item-based collaborative filtering to predict if a user will like or dislike a video?\"\n",
      "What is the problem with recommending videos to users using one of the collaborative filtering techniques discussed?\n",
      "\"What is matrix factorization and why is it used to reduce the dimensions of the user-video matrix?\"\n",
      "What are the advantages and disadvantages of user-user and item-item collaborative filtering? What are the pros of matrix factorization?\n",
      "\"What are the seven features that make up the user context and why are they significant?\"\n",
      "What is the input and output of a neural network in the context of a video recommender?\n",
      "What is the purpose of constructing the input vector X?\n",
      "\"What is the ranking algorithm used by YouTube to predict expected watch time and why is it causing a number of creators to complain?\"\n",
      "\"What is the relevance of the score generated by YouTube's algorithm and the deep learning approach for building a recommender system? Why does this work?\"\n",
      "\"What are the disadvantages of using collaborative filtering in general and what are the solutions to these disadvantages?\"\n",
      "\n",
      "\"What did you learn from the video about recommender systems and YouTube's deep learning techniques? Why should viewers like and subscribe to the video?\"\n"
     ]
    }
   ],
   "source": [
    "#get summary, if there are multiple chunks loop through all the chunks and club them to form the summary\n",
    "llm = OpenAI(temperature=0,openai_api_key=\"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\")\n",
    "transcriptSummary = \"\"\n",
    "headingList = []\n",
    "\n",
    "template = \"\"\"\n",
    "Given a body of text in Transcript, Generate an appropriate prompt text that can used to get a crisp output what , why from the Transcript. Output only the prompt text and prefix the prompt text with 'Generating Prompt :'.\"\n",
    "\n",
    "Transcript:\n",
    "{content}\n",
    "\n",
    "\"\"\"\n",
    "# template = \"\"\"\n",
    "# Given a body of text in Transcript, Generate a heading pertaining to core part of Transcript text. Prefix the heading with 'Generating Heading'. Output only the generated heading\"\n",
    "\n",
    "# Transcript:\n",
    "# {content}\n",
    "\n",
    "# \"\"\"\n",
    "\n",
    "# template = \"\"\"\n",
    "# Given a body of text in Transcript, you will output the crisp summary covering the important points.\"\n",
    "\n",
    "# Transcript:\n",
    "# {content}\n",
    "\n",
    "# \"\"\"\n",
    "for trnscpt in inputChunks:\n",
    "    if len(trnscpt) > 300:\n",
    "\n",
    "        prompt = PromptTemplate(template=template, input_variables=[\"content\"])\n",
    "\n",
    "        # fill the prompt template\n",
    "        prompt_text = prompt.format(content = trnscpt)\n",
    "        chunkSummary = llm(prompt_text ).replace('\\nGenerating Prompt: ','')\n",
    "        #chunkSummary = chunkSummary + \". Generate 3 bullet points.\"\n",
    "        headingList.append(chunkSummary)\n",
    "        print (chunkSummary)\n",
    "        # if (chunkSummary != \"\" ) :\n",
    "        #     transcriptSummary = transcriptSummary +  chunkSummary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n\"What is the nature of YouTube\\'s recommendation system and why does it optimize watch time?\"',\n",
       " 'What is the attack on the YouTube algorithm mentioned in the video and how does it affect the algorithm?',\n",
       " '\"What is the intuition behind collaborative filtering and how is it used to make predictions for newer users and items?\"',\n",
       " '\\nWhat is the goal of computing the similarities between the user vectors and User D?',\n",
       " '\"What are the pros and cons of user-based collaborative filtering and how does item-based collaborative filtering compare? How can we use item-based collaborative filtering to predict if a user will like or dislike a video?\"',\n",
       " 'What is the problem with recommending videos to users using one of the collaborative filtering techniques discussed?',\n",
       " '\"What is matrix factorization and why is it used to reduce the dimensions of the user-video matrix?\"',\n",
       " 'What are the advantages and disadvantages of user-user and item-item collaborative filtering? What are the pros of matrix factorization?',\n",
       " '\"What are the seven features that make up the user context and why are they significant?\"',\n",
       " 'What is the input and output of a neural network in the context of a video recommender?',\n",
       " 'What is the purpose of constructing the input vector X?',\n",
       " '\"What is the ranking algorithm used by YouTube to predict expected watch time and why is it causing a number of creators to complain?\"',\n",
       " '\"What is the relevance of the score generated by YouTube\\'s algorithm and the deep learning approach for building a recommender system? Why does this work?\"',\n",
       " '\"What are the disadvantages of using collaborative filtering in general and what are the solutions to these disadvantages?\"',\n",
       " '\\n\"What did you learn from the video about recommender systems and YouTube\\'s deep learning techniques? Why should viewers like and subscribe to the video?\"']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headingList \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicate heading\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "headingDF = pd.DataFrame({'headingtext': headingList})\n",
    "newheadingDF = pd.DataFrame()\n",
    "newheadingDF['headingembedding'] = headingDF.headingtext.apply(lambda x: get_embedding(x, model='text-embedding-ada-002'))\n",
    "\n",
    "similarity = cosine_similarity(newheadingDF)\n",
    "print(similarity)\n",
    "# Generate embeddings for each of them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "similarity = cosine_similarity(newheadingDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summarize into 3 bullet points. Generate headings for each bullet points\n",
    "llm = OpenAI(temperature=0,openai_api_key=\"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\")\n",
    "template = \"\"\"\n",
    "    From the Transcript text, Generate 6 important headings pertaining to the Transcript text that is worth learning about in detail. Concatenate the headings with a ~ seperator. \n",
    "    \n",
    "\n",
    "    Transcript:\n",
    "    {content}\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"content\"])\n",
    "\n",
    "# fill the prompt template\n",
    "prompt_text = prompt.format(content = transcriptSummary)\n",
    "chunkSummary = llm(prompt_text )\n",
    "headingList = chunkSummary.split(\"~\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\"What is the nature of YouTube's recommendation system and why does it optimize watch time?\"\n",
      "\n",
      "1. YouTube's recommendation system is powered by AI.\n",
      "2. It looks at user's past activity to suggest videos.\n",
      "3. It tries to figure out what videos would keep users on the platform the longest.\n",
      "4. It optimizes watch time by suggesting videos that users would be interested in.\n",
      "5. It is constantly learning and adapting to user's preferences.\n",
      "What is the attack on the YouTube algorithm mentioned in the video and how does it affect the algorithm?\n",
      "\n",
      "• Attack on YouTube Algorithm: Tailoring videos to appeal to the algorithm, even if the video is of low quality.\n",
      "• Effect on Algorithm: Algorithm is picking up and recommending low quality videos to viewers.\n",
      "• Matrix used: Sparse matrix with empty values (labeled as zero).\n",
      "• Predictions: Predictive user D will like or dislike video three.\n",
      "• Recommendation: Watch the video for full scoop.\n",
      "\"What is the intuition behind collaborative filtering and how is it used to make predictions for newer users and items?\"\n",
      "\n",
      "\n",
      "- Intuition behind collaborative filtering is collaboration. \n",
      "- We use the ratings of older users to make predictions for newer users. \n",
      "- We can use the ratings on older items to make predictions for newer items. \n",
      "- User-user collaborative filtering and item-item collaborative filtering are two types of collaborative filtering. \n",
      "- User-user collaborative filtering takes into account the ratings of other users to make predictions for newer users.\n",
      "\n",
      "What is the goal of computing the similarities between the user vectors and User D?\n",
      "\n",
      "- The goal of computing the similarities between the user vectors and User D is to determine if User D will like Video Three. \n",
      "- This way, we can determine whether we should recommend the video to him or not. \n",
      "- We normalize the user vectors already and use Cosine Similarity to compute the similarities. \n",
      "- Computing similarities is to determine which users are more in line with User D, so that we can make recommendations based on those users. \n",
      "- We can observe that Users A and D tend to have opposite tastes.\n",
      "\"What are the pros and cons of user-based collaborative filtering and how does item-based collaborative filtering compare? How can we use item-based collaborative filtering to predict if a user will like or dislike a video?\"\n",
      "\n",
      "Answer:\n",
      "\n",
      "Pros of User-based Collaborative Filtering: \n",
      "• Simple to implement \n",
      "• Easy to understand \n",
      "\n",
      "Cons of User-based Collaborative Filtering: \n",
      "• Does not scale well \n",
      "\n",
      "Item-based Collaborative Filtering: \n",
      "• Finds cosine similarities between items \n",
      "• Can be used to predict if a user will like or dislike a video \n",
      "\n",
      "Advantages of Item-based Collaborative Filtering: \n",
      "• Easier to scale than user-based collaborative filtering \n",
      "• Can be used to make more accurate predictions \n",
      "• Can be used to recommend items to users\n",
      "What is the problem with recommending videos to users using one of the collaborative filtering techniques discussed?\n",
      "\n",
      "- Collaborative filtering techniques require a lot of data to be effective.\n",
      "- The data points have no information on most of the axes.\n",
      "- The user and item vectors need to be projected onto a much smaller space.\n",
      "- This reduces the dimensions and increases computation power.\n",
      "- This can lead to inaccurate recommendations.\n",
      "\"What is matrix factorization and why is it used to reduce the dimensions of the user-video matrix?\"\n",
      "\n",
      "• Matrix factorization is a technique used to reduce the dimensions of the user-video matrix.\n",
      "• It is similar to PCA's dimensionality reduction in machine learning.\n",
      "• The user-video matrix has a lot of zeroes, so by projecting it onto a smaller space, the computation efficiency is increased.\n",
      "• Both users and videos are projected onto the same space, so they can be compared directly.\n",
      "• This helps to make new business decisions and identify improvements to increase revenue.\n",
      "What are the advantages and disadvantages of user-user and item-item collaborative filtering? What are the pros of matrix factorization?\n",
      "\n",
      "Advantages of User-User Collaborative Filtering:\n",
      "- Simple to implement\n",
      "- Recommends videos to a user based on similar users\n",
      "\n",
      "Disadvantages of User-User Collaborative Filtering:\n",
      "- Does not scale well with large number of users and videos\n",
      "\n",
      "Advantages of Item-Item Collaborative Filtering:\n",
      "- Simple to implement\n",
      "- Recommends videos based on similar videos that a user liked\n",
      "\n",
      "Disadvantages of Item-Item Collaborative Filtering:\n",
      "- Does not scale well with large number of users and videos\n",
      "\n",
      "Pros of Matrix Factorization:\n",
      "- Predictions are independent of the number of users and videos\n",
      "- Scales better than collaborative filtering\n",
      "\"What are the seven features that make up the user context and why are they significant?\"\n",
      "\n",
      "\n",
      "- User: Information about the user such as age, gender, etc.\n",
      "- Item: Information about the item such as video freshness, etc.\n",
      "- Latent Space: An unknown space where users and items are transposed.\n",
      "- Device Type: Information about the device used to view the video.\n",
      "- Location: Information about the user's location.\n",
      "- Video Freshness: Age of the video.\n",
      "- Neural Network: A way to feed the user context into a neural network.\n",
      "What is the input and output of a neural network in the context of a video recommender?\n",
      "\n",
      "Input: User features, user context \n",
      "\n",
      "Output: Set of recommended videos \n",
      "\n",
      "Bullet Points: \n",
      "1. Neural network takes user features and user context as input \n",
      "2. Output is a set of recommended videos \n",
      "3. Training data is collected from user activity over time \n",
      "4. Every time a user clicks on a video, a training sample is documented \n",
      "5. Training samples consist of user context and the videos watched\n",
      "What is the purpose of constructing the input vector X?\n",
      "\n",
      "- To construct the input vector X for every user context.\n",
      "- To determine a set of videos to recommend to the person, which is called S.\n",
      "- To find the optimal set of videos that maximizes the equation.\n",
      "- To rank the videos based on the user's watch history, search history, location, and other information.\n",
      "- To get the appropriate set of videos to recommend a user.\n",
      "\"What is the ranking algorithm used by YouTube to predict expected watch time and why is it causing a number of creators to complain?\"\n",
      "\n",
      "Answer:\n",
      "\n",
      "1. YouTube uses a ranking algorithm to predict expected watch time.\n",
      "2. The scores are directly proportional to watch time.\n",
      "3. The ranking algorithm will likely recommend longer videos.\n",
      "4. YouTube uses a technique called weighted logistic regression to come up with these scores.\n",
      "5. This is causing a number of creators to complain.\n",
      "\"What is the relevance of the score generated by YouTube's algorithm and the deep learning approach for building a recommender system? Why does this work?\"\n",
      "\n",
      "• YouTube's algorithm and deep learning approach for building a recommender system assigns a relevant score to a video based on hundreds of features of the video and user input.\n",
      "• This score is also the expected watch time of the video.\n",
      "• The algorithm works by assigning a high score to a video if it believes the user will like it.\n",
      "• This score is the relevant score for the given user.\n",
      "• Collaborative filtering uses ratings of different users to predict if a particular user will like a video.\n",
      "\"What are the disadvantages of using collaborative filtering in general and what are the solutions to these disadvantages?\"\n",
      "\n",
      "• New users joining the platform at a high rate can be a disadvantage of using collaborative filtering. \n",
      "• Solution: Use videos instead of users to make predictions.\n",
      "• Sparsity can be a problem with collaborative filtering, leading to too much storage space and computation complexity.\n",
      "• Solution: Use the second major technique to reduce sparsity.\n",
      "• Solution: Use watch time to generate rankings of videos.\n",
      "\n",
      "\"What did you learn from the video about recommender systems and YouTube's deep learning techniques? Why should viewers like and subscribe to the video?\"\n",
      "\n",
      "• Recommender systems in general and YouTube's deep learning techniques were discussed in the video. \n",
      "• Viewers should like and subscribe to the video as it took a long time to make. \n",
      "• Subscribing will help viewers stay up to date with the content. \n",
      "• Liking and subscribing will be appreciated by the creator. \n",
      "• The video concluded with a goodbye.\n"
     ]
    }
   ],
   "source": [
    "#get summary of the text if there are not chunks\n",
    "for headingText in headingList:\n",
    "    # Extract the relevant contents from pinecone\n",
    "    # generate embeddedings for the heading\n",
    "    print(headingText)\n",
    "    headingcontent = inputChunks[headingList.index(headingText)]\n",
    "    # if headingText != \"\":\n",
    "    #     headingembedding = get_embedding(headingText, model='text-embedding-ada-002')\n",
    "    # vector_database_results_matching = pindex.query(headingembedding, top_k=5, include_metadata=True, include_Values=True, \n",
    "    #     namespace=\"youtubeindex\")\n",
    "    # for match in vector_database_results_matching['matches']:\n",
    "    #     if float(match['score']) *100 > 75 :\n",
    "    #         headingcontent = headingcontent + \" \" + match['metadata']['combined']\n",
    "    #  headingcontent = headingText.index       \n",
    "    # Generate questions gives the combined content\n",
    "    if headingcontent != \"\":\n",
    "\n",
    "        template = \"\"\"Given the text in Trascript. {question}. Output the answers in simple 5 bullet points. Keep the bullet points crisp.\n",
    "\n",
    "        Transcript:\n",
    "        {content}\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        # template = \"\"\"\n",
    "        # From the Transcript text, Generate flashcard text as bullet points on aspects dealing with why and what is the significace, how to, examples and analogy. Flashcard text should not contain questions. As much as possible make the flashcard unique. Remove any  duplicates\n",
    "        # Transcript:\n",
    "        # {content}\n",
    "\n",
    "        # \"\"\"\n",
    "        prompt = PromptTemplate(template=template, input_variables=[\"content\",\"question\"])\n",
    "\n",
    "        # fill the prompt template\n",
    "        prompt_text = prompt.format(content = headingcontent, question = headingText)\n",
    "        flashcardSummary = llm(prompt_text )\n",
    "        print (flashcardSummary)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#With the summary, get the bullet list of heading.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with extracted contents for each heading - generate flashcards for each heading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#given a chunk, output the summary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
