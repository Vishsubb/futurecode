{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# make necessary imports of the libraries\n",
    "!pip install youtube-transcript-api\n",
    "!pip install openai\n",
    "!pip install pandas\n",
    "!pip install langchain\n",
    "!pip install pinecone-client\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "# Replace 'YOUR_OPENAI_API_KEY' with your actual API key\n",
    "openai.api_key = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_section =\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gettranscriptfromvideo(video_id, filename=\"\"):\n",
    "\n",
    "    # Get the transcript for a video\n",
    "    transcripttext = YouTubeTranscriptApi.get_transcript(video_id)\n",
    "\n",
    "    #format the transcript got from the youtube video\n",
    "    formatter = TextFormatter()\n",
    "    transcript = formatter.format_transcript(transcripttext)\n",
    "    if filename:\n",
    "        #write the transcript to a file\n",
    "        with open(filename + '.txt', 'w') as f:\n",
    "            f.write(transcript)\n",
    "        #find length of the transcript\n",
    "        length = len(transcript)\n",
    "        #if length is zero then print no transcript found\n",
    "        if length == 0:\n",
    "            print(\"No transcript found\")\n",
    "        else:\n",
    "            print(\"Transcript found and written to file\")\n",
    "    return transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text, model=\"text-embedding-ada-002\"):\n",
    "   text = text.replace(\"\\n\", \" \")\n",
    "   return openai.Embedding.create(input = [text], model=model)['data'][0]['embedding']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 26)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m<tokenize>:26\u001b[1;36m\u001b[0m\n\u001b[1;33m    answertext = llm(prompt_text )\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "def get_summary(chunks):\n",
    "\n",
    "    llm = OpenAI(temperature=0.4,openai_api_key=\"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\")\n",
    "    summary_chunks=[]    \n",
    "    template = \"\"\"\n",
    "    Summarise content in the context section. Use content in the previous section for continuity. Please dont state it as summary. Highlight important points. Keep the narration coversational\"\n",
    "\n",
    "    Context section:\n",
    "    {content}\n",
    "\n",
    "    \n",
    "    previous section:\n",
    "    {prev_section}\n",
    "\n",
    "   \"\"\"\n",
    "    prompt = PromptTemplate(template=template, input_variables=[\"content\", \"prev_section\"])\n",
    "    #prompt = PromptTemplate(template=template, input_variables=[\"content\"])\n",
    "\n",
    "    prev_section =\"\"\n",
    "    for text in chunks:\n",
    "        if (len(text) > 5):\n",
    "             prompt_text = prompt.format(content = text, prev_section = prev_section)\n",
    "            #prompt_text = prompt.format(content = text)\n",
    "            # prev_section = llm(prompt_text ) \n",
    "            # print (prev_section)\n",
    "            answertext = llm(prompt_text )\n",
    "            prev_section = answertext\n",
    "            summary_chunks.append(llm(answertext ))\n",
    "    return(summary_chunks)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getembeddingsandsavetovectordb(chunks,video_id, dbindex,vectordbkey ):\n",
    "    text_chunks=[]\n",
    "\n",
    "    for text in chunks:\n",
    "        text_chunks.append(text)\n",
    "\n",
    "    df = pd.DataFrame({'text_chunks': text_chunks})\n",
    "\n",
    "    #loop through the chunks and call function get_embedding to get embeddings for each chunk\n",
    "    df['ada_embedding'] = df.text_chunks.apply(lambda x: get_embedding(x, model='text-embedding-ada-002'))\n",
    "\n",
    "    #store the embeddings in pinecone\n",
    "    pinecone.init(\n",
    "        api_key=vectordbkey,\n",
    "        environment=\"us-west1-gcp-free\")\n",
    "\n",
    "    pindex = pinecone.Index(dbindex)\n",
    "\n",
    "    meta= [{'combined': line} for line in df['text_chunks']]\n",
    "    vectors = list(zip(video_id, df['ada_embedding'], meta))\n",
    "\n",
    "    upsert_response = pindex.upsert(\n",
    "        vectors=vectors,\n",
    "        namespace=dbindex, values=True, include_metadata=True)\n",
    "    return pindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def savetranscriptvectorfromvideo():\n",
    "    video_id = input(\"Enter the video id: \")\n",
    "    chunks = split_transcript_into_chunks (gettranscriptfromvideo(video_id),1000)\n",
    "    print (len(chunks))\n",
    "    #get_summary(chunks)\n",
    "    #vectordbindex = getembeddingsandsavetovectordb(chunks,video_id, \"myyoutubeindex\",\"df750bd6-bfd5-4298-862c-0f9cdfaee6bc\" )\n",
    "    return vectordbindex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk length\n",
      "getting summary...\n",
      "storing in vectordb..\n",
      "first\n",
      "second  in je Advanced you\n",
      "should definitely try out black book\n",
      "man so much of information right are\n",
      "they still here\n",
      "are you alive if yes if you are still\n",
      "here if you're still following the video\n",
      "I want you to go to the comments below\n",
      "and ask any doubts that you have about\n",
      "whatever we discussed in love and I\n",
      "promise that I'll definitely reply to\n",
      "them and try my best to solve your\n",
      "doubts pause the video right now go ask\n",
      "your doubts right now so now as we are\n",
      "done with the syllabus important topics\n",
      "and the best books for each of these\n",
      "subjects let's get to the next most\n",
      "important set of things the first one\n",
      "the best two-year entire timeline for\n",
      "your ji preparation the best daily\n",
      "schedule to maximize your potential and\n",
      "some of the most important things that\n",
      "every student has to remember so that\n",
      "they don't end the wasting two years of\n",
      "the preparation I've covered all these\n",
      "three points in the next part of this\n",
      "video series as I thought this video is\n",
      "getting lengthy and that second part is\n",
      "going t e'll also discuss\n",
      "about how you can make the best daily\n",
      "timetable and trust me this is going to\n",
      "be a game changer and at the end of this\n",
      "series we'll be talking about some of\n",
      "the most important things that a lot of\n",
      "students miss out while preparing for\n",
      "iitjee in simple words by the end of\n",
      "this series you'll know everything that\n",
      "you need to know from what to study to\n",
      "how to study you'll have the entire\n",
      "roadmap for idja\n",
      "[Music]\n",
      "the first step in the plan is to\n",
      "understand the syllabus properly listen\n",
      "a lot of students actually miss out on\n",
      "this and that's why they finally make\n",
      "very inefficient plans which end up\n",
      "wasting a lot of time going forward but\n",
      "if you are aware of the syllabus which\n",
      "is exclusive for Jee Mains and Je\n",
      "Advanced you can actually save a lot of\n",
      "time let's get started with physics this\n",
      "image that you see on the screen right\n",
      "now is the entire syllabus go back two\n",
      "seconds and take a screenshot of it\n",
      "because you can refer to it later a lot\n",
      "of people actually get scared of phy\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "from youtube_transcript_api.formatters import TextFormatter\n",
    "from langchain import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "import requests\n",
    "import pinecone\n",
    "\n",
    "video_id = input(\"Enter the video id: \")\n",
    "chunks = split_transcript_into_chunks (gettranscriptfromvideo(video_id),500)\n",
    "print (\"Chunk length\")\n",
    "print (\"getting summary...\")\n",
    "df = pd.DataFrame({'summary_chunks': get_summary(chunks)})\n",
    "print (\"storing in vectordb..\")\n",
    "vectordbindex = getembeddingsandsavetovectordb(chunks,video_id, \"myyoutubeindex\",\"df750bd6-bfd5-4298-862c-0f9cdfaee6bc\" )\n",
    "answertext = returnresultforquestion(vectordbindex)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"every weekday I wake up at 6 00 am I\\nhaven't set an alarm for a long time but\\nI know that my body will naturally wake\\nme up based on the routine that I've\\ncreated I usually sleep for about eight\\nhours a night and I'm very grateful to\\nhave that much time and I prioritize\\nsleeping at about 10 pm every single day\\nwhen I'm up the first thing my mind\\nwants to do is say something like I feel\\nso tired I've got so much to do today\\nand I as soon as I wake up in that\\nmoment rewiring reprogram it to say I \""
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[206], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mprint\u001b[39m(get_summary(chunks)[\u001b[39m0\u001b[39m])\n",
      "Cell \u001b[1;32mIn[184], line 26\u001b[0m, in \u001b[0;36mget_summary\u001b[1;34m(chunks)\u001b[0m\n\u001b[0;32m     23\u001b[0m         prompt_text \u001b[39m=\u001b[39m prompt\u001b[39m.\u001b[39mformat(content \u001b[39m=\u001b[39m text)\n\u001b[0;32m     24\u001b[0m         \u001b[39m# prev_section = llm(prompt_text ) \u001b[39;00m\n\u001b[0;32m     25\u001b[0m         \u001b[39m# print (prev_section)\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m         answertext \u001b[39m=\u001b[39m llm(prompt_text )\n\u001b[0;32m     27\u001b[0m         summary_chunks\u001b[39m.\u001b[39mappend(llm(answertext ))\n\u001b[0;32m     28\u001b[0m \u001b[39mreturn\u001b[39;00m(summary_chunks)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\langchain\\llms\\base.py:786\u001b[0m, in \u001b[0;36mBaseLLM.__call__\u001b[1;34m(self, prompt, stop, callbacks, tags, metadata, **kwargs)\u001b[0m\n\u001b[0;32m    779\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(prompt, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    780\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    781\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mArgument `prompt` is expected to be a string. Instead found \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    782\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(prompt)\u001b[39m}\u001b[39;00m\u001b[39m. If you want to run the LLM on multiple prompts, use \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    783\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`generate` instead.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    784\u001b[0m     )\n\u001b[0;32m    785\u001b[0m \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m--> 786\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgenerate(\n\u001b[0;32m    787\u001b[0m         [prompt],\n\u001b[0;32m    788\u001b[0m         stop\u001b[39m=\u001b[39mstop,\n\u001b[0;32m    789\u001b[0m         callbacks\u001b[39m=\u001b[39mcallbacks,\n\u001b[0;32m    790\u001b[0m         tags\u001b[39m=\u001b[39mtags,\n\u001b[0;32m    791\u001b[0m         metadata\u001b[39m=\u001b[39mmetadata,\n\u001b[0;32m    792\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m    793\u001b[0m     )\n\u001b[0;32m    794\u001b[0m     \u001b[39m.\u001b[39mgenerations[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]\n\u001b[0;32m    795\u001b[0m     \u001b[39m.\u001b[39mtext\n\u001b[0;32m    796\u001b[0m )\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\langchain\\llms\\base.py:582\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[1;34m(self, prompts, stop, callbacks, tags, metadata, **kwargs)\u001b[0m\n\u001b[0;32m    573\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    574\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    575\u001b[0m         )\n\u001b[0;32m    576\u001b[0m     run_managers \u001b[39m=\u001b[39m [\n\u001b[0;32m    577\u001b[0m         callback_manager\u001b[39m.\u001b[39mon_llm_start(\n\u001b[0;32m    578\u001b[0m             dumpd(\u001b[39mself\u001b[39m), [prompt], invocation_params\u001b[39m=\u001b[39mparams, options\u001b[39m=\u001b[39moptions\n\u001b[0;32m    579\u001b[0m         )[\u001b[39m0\u001b[39m]\n\u001b[0;32m    580\u001b[0m         \u001b[39mfor\u001b[39;00m callback_manager, prompt \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(callback_managers, prompts)\n\u001b[0;32m    581\u001b[0m     ]\n\u001b[1;32m--> 582\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate_helper(\n\u001b[0;32m    583\u001b[0m         prompts, stop, run_managers, \u001b[39mbool\u001b[39m(new_arg_supported), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m    584\u001b[0m     )\n\u001b[0;32m    585\u001b[0m     \u001b[39mreturn\u001b[39;00m output\n\u001b[0;32m    586\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(missing_prompts) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\langchain\\llms\\base.py:488\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    486\u001b[0m     \u001b[39mfor\u001b[39;00m run_manager \u001b[39min\u001b[39;00m run_managers:\n\u001b[0;32m    487\u001b[0m         run_manager\u001b[39m.\u001b[39mon_llm_error(e)\n\u001b[1;32m--> 488\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m    489\u001b[0m flattened_outputs \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mflatten()\n\u001b[0;32m    490\u001b[0m \u001b[39mfor\u001b[39;00m manager, flattened_output \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\langchain\\llms\\base.py:475\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    465\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_generate_helper\u001b[39m(\n\u001b[0;32m    466\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    467\u001b[0m     prompts: List[\u001b[39mstr\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    471\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[0;32m    472\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m LLMResult:\n\u001b[0;32m    473\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    474\u001b[0m         output \u001b[39m=\u001b[39m (\n\u001b[1;32m--> 475\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate(\n\u001b[0;32m    476\u001b[0m                 prompts,\n\u001b[0;32m    477\u001b[0m                 stop\u001b[39m=\u001b[39mstop,\n\u001b[0;32m    478\u001b[0m                 \u001b[39m# TODO: support multiple run managers\u001b[39;00m\n\u001b[0;32m    479\u001b[0m                 run_manager\u001b[39m=\u001b[39mrun_managers[\u001b[39m0\u001b[39m] \u001b[39mif\u001b[39;00m run_managers \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    480\u001b[0m                 \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m    481\u001b[0m             )\n\u001b[0;32m    482\u001b[0m             \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    483\u001b[0m             \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate(prompts, stop\u001b[39m=\u001b[39mstop)\n\u001b[0;32m    484\u001b[0m         )\n\u001b[0;32m    485\u001b[0m     \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    486\u001b[0m         \u001b[39mfor\u001b[39;00m run_manager \u001b[39min\u001b[39;00m run_managers:\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\langchain\\llms\\openai.py:399\u001b[0m, in \u001b[0;36mBaseOpenAI._generate\u001b[1;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    387\u001b[0m     choices\u001b[39m.\u001b[39mappend(\n\u001b[0;32m    388\u001b[0m         {\n\u001b[0;32m    389\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m: generation\u001b[39m.\u001b[39mtext,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    396\u001b[0m         }\n\u001b[0;32m    397\u001b[0m     )\n\u001b[0;32m    398\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 399\u001b[0m     response \u001b[39m=\u001b[39m completion_with_retry(\n\u001b[0;32m    400\u001b[0m         \u001b[39mself\u001b[39m, prompt\u001b[39m=\u001b[39m_prompts, run_manager\u001b[39m=\u001b[39mrun_manager, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[0;32m    401\u001b[0m     )\n\u001b[0;32m    402\u001b[0m     choices\u001b[39m.\u001b[39mextend(response[\u001b[39m\"\u001b[39m\u001b[39mchoices\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m    403\u001b[0m     update_token_usage(_keys, response, token_usage)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\langchain\\llms\\openai.py:115\u001b[0m, in \u001b[0;36mcompletion_with_retry\u001b[1;34m(llm, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[39m@retry_decorator\u001b[39m\n\u001b[0;32m    112\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_completion_with_retry\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m    113\u001b[0m     \u001b[39mreturn\u001b[39;00m llm\u001b[39m.\u001b[39mclient\u001b[39m.\u001b[39mcreate(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 115\u001b[0m \u001b[39mreturn\u001b[39;00m _completion_with_retry(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\tenacity\\__init__.py:289\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(f)\n\u001b[0;32m    288\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped_f\u001b[39m(\u001b[39m*\u001b[39margs: t\u001b[39m.\u001b[39mAny, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: t\u001b[39m.\u001b[39mAny) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m t\u001b[39m.\u001b[39mAny:\n\u001b[1;32m--> 289\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m(f, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\tenacity\\__init__.py:379\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    377\u001b[0m retry_state \u001b[39m=\u001b[39m RetryCallState(retry_object\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m, fn\u001b[39m=\u001b[39mfn, args\u001b[39m=\u001b[39margs, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[0;32m    378\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m--> 379\u001b[0m     do \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49miter(retry_state\u001b[39m=\u001b[39;49mretry_state)\n\u001b[0;32m    380\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(do, DoAttempt):\n\u001b[0;32m    381\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\tenacity\\__init__.py:314\u001b[0m, in \u001b[0;36mBaseRetrying.iter\u001b[1;34m(self, retry_state)\u001b[0m\n\u001b[0;32m    312\u001b[0m is_explicit_retry \u001b[39m=\u001b[39m fut\u001b[39m.\u001b[39mfailed \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(fut\u001b[39m.\u001b[39mexception(), TryAgain)\n\u001b[0;32m    313\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (is_explicit_retry \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mretry(retry_state)):\n\u001b[1;32m--> 314\u001b[0m     \u001b[39mreturn\u001b[39;00m fut\u001b[39m.\u001b[39;49mresult()\n\u001b[0;32m    316\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mafter \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mafter(retry_state)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\concurrent\\futures\\_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    449\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    450\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[1;32m--> 451\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__get_result()\n\u001b[0;32m    453\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_condition\u001b[39m.\u001b[39mwait(timeout)\n\u001b[0;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[1;32mc:\\Python310\\lib\\concurrent\\futures\\_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    401\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception:\n\u001b[0;32m    402\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 403\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception\n\u001b[0;32m    404\u001b[0m     \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    405\u001b[0m         \u001b[39m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    406\u001b[0m         \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\tenacity\\__init__.py:382\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(do, DoAttempt):\n\u001b[0;32m    381\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 382\u001b[0m         result \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    383\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m:  \u001b[39m# noqa: B902\u001b[39;00m\n\u001b[0;32m    384\u001b[0m         retry_state\u001b[39m.\u001b[39mset_exception(sys\u001b[39m.\u001b[39mexc_info())  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\langchain\\llms\\openai.py:113\u001b[0m, in \u001b[0;36mcompletion_with_retry.<locals>._completion_with_retry\u001b[1;34m(**kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[39m@retry_decorator\u001b[39m\n\u001b[0;32m    112\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_completion_with_retry\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m--> 113\u001b[0m     \u001b[39mreturn\u001b[39;00m llm\u001b[39m.\u001b[39mclient\u001b[39m.\u001b[39mcreate(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\openai\\api_resources\\completion.py:25\u001b[0m, in \u001b[0;36mCompletion.create\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m     24\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 25\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mcreate(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     26\u001b[0m     \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     27\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m start \u001b[39m+\u001b[39m timeout:\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[1;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[0;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[0;32m    129\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[0;32m    137\u001b[0m ):\n\u001b[0;32m    138\u001b[0m     (\n\u001b[0;32m    139\u001b[0m         deployment_id,\n\u001b[0;32m    140\u001b[0m         engine,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[0;32m    151\u001b[0m     )\n\u001b[1;32m--> 153\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[0;32m    154\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    155\u001b[0m         url,\n\u001b[0;32m    156\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[0;32m    157\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m    158\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[0;32m    159\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[0;32m    160\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[0;32m    161\u001b[0m     )\n\u001b[0;32m    163\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[0;32m    164\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n\u001b[0;32m    165\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\openai\\api_requestor.py:288\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[1;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[0;32m    278\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    279\u001b[0m     method,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    286\u001b[0m     request_timeout: Optional[Union[\u001b[39mfloat\u001b[39m, Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    287\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[1;32m--> 288\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest_raw(\n\u001b[0;32m    289\u001b[0m         method\u001b[39m.\u001b[39;49mlower(),\n\u001b[0;32m    290\u001b[0m         url,\n\u001b[0;32m    291\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[0;32m    292\u001b[0m         supplied_headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m    293\u001b[0m         files\u001b[39m=\u001b[39;49mfiles,\n\u001b[0;32m    294\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[0;32m    295\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[0;32m    296\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[0;32m    297\u001b[0m     )\n\u001b[0;32m    298\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response(result, stream)\n\u001b[0;32m    299\u001b[0m     \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\openai\\api_requestor.py:596\u001b[0m, in \u001b[0;36mAPIRequestor.request_raw\u001b[1;34m(self, method, url, params, supplied_headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[0;32m    594\u001b[0m     _thread_context\u001b[39m.\u001b[39msession_create_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m    595\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 596\u001b[0m     result \u001b[39m=\u001b[39m _thread_context\u001b[39m.\u001b[39;49msession\u001b[39m.\u001b[39;49mrequest(\n\u001b[0;32m    597\u001b[0m         method,\n\u001b[0;32m    598\u001b[0m         abs_url,\n\u001b[0;32m    599\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m    600\u001b[0m         data\u001b[39m=\u001b[39;49mdata,\n\u001b[0;32m    601\u001b[0m         files\u001b[39m=\u001b[39;49mfiles,\n\u001b[0;32m    602\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[0;32m    603\u001b[0m         timeout\u001b[39m=\u001b[39;49mrequest_timeout \u001b[39mif\u001b[39;49;00m request_timeout \u001b[39melse\u001b[39;49;00m TIMEOUT_SECS,\n\u001b[0;32m    604\u001b[0m         proxies\u001b[39m=\u001b[39;49m_thread_context\u001b[39m.\u001b[39;49msession\u001b[39m.\u001b[39;49mproxies,\n\u001b[0;32m    605\u001b[0m     )\n\u001b[0;32m    606\u001b[0m \u001b[39mexcept\u001b[39;00m requests\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mTimeout \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    607\u001b[0m     \u001b[39mraise\u001b[39;00m error\u001b[39m.\u001b[39mTimeout(\u001b[39m\"\u001b[39m\u001b[39mRequest timed out: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(e)) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msend(prep, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39msend(request, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    705\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\requests\\adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    483\u001b[0m     timeout \u001b[39m=\u001b[39m TimeoutSauce(connect\u001b[39m=\u001b[39mtimeout, read\u001b[39m=\u001b[39mtimeout)\n\u001b[0;32m    485\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 486\u001b[0m     resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[0;32m    487\u001b[0m         method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[0;32m    488\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[0;32m    489\u001b[0m         body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[0;32m    490\u001b[0m         headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[0;32m    491\u001b[0m         redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    492\u001b[0m         assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    493\u001b[0m         preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    494\u001b[0m         decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    495\u001b[0m         retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[0;32m    496\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[0;32m    497\u001b[0m         chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[0;32m    498\u001b[0m     )\n\u001b[0;32m    500\u001b[0m \u001b[39mexcept\u001b[39;00m (ProtocolError, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m    501\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(err, request\u001b[39m=\u001b[39mrequest)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\urllib3\\connectionpool.py:790\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    787\u001b[0m response_conn \u001b[39m=\u001b[39m conn \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m release_conn \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    789\u001b[0m \u001b[39m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 790\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_request(\n\u001b[0;32m    791\u001b[0m     conn,\n\u001b[0;32m    792\u001b[0m     method,\n\u001b[0;32m    793\u001b[0m     url,\n\u001b[0;32m    794\u001b[0m     timeout\u001b[39m=\u001b[39mtimeout_obj,\n\u001b[0;32m    795\u001b[0m     body\u001b[39m=\u001b[39mbody,\n\u001b[0;32m    796\u001b[0m     headers\u001b[39m=\u001b[39mheaders,\n\u001b[0;32m    797\u001b[0m     chunked\u001b[39m=\u001b[39mchunked,\n\u001b[0;32m    798\u001b[0m     retries\u001b[39m=\u001b[39mretries,\n\u001b[0;32m    799\u001b[0m     response_conn\u001b[39m=\u001b[39mresponse_conn,\n\u001b[0;32m    800\u001b[0m     preload_content\u001b[39m=\u001b[39mpreload_content,\n\u001b[0;32m    801\u001b[0m     decode_content\u001b[39m=\u001b[39mdecode_content,\n\u001b[0;32m    802\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mresponse_kw,\n\u001b[0;32m    803\u001b[0m )\n\u001b[0;32m    805\u001b[0m \u001b[39m# Everything went great!\u001b[39;00m\n\u001b[0;32m    806\u001b[0m clean_exit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\urllib3\\connectionpool.py:536\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    534\u001b[0m \u001b[39m# Receive the response from the server\u001b[39;00m\n\u001b[0;32m    535\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 536\u001b[0m     response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49mgetresponse()\n\u001b[0;32m    537\u001b[0m \u001b[39mexcept\u001b[39;00m (BaseSSLError, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    538\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_timeout(err\u001b[39m=\u001b[39me, url\u001b[39m=\u001b[39murl, timeout_value\u001b[39m=\u001b[39mread_timeout)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\urllib3\\connection.py:461\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    458\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mresponse\u001b[39;00m \u001b[39mimport\u001b[39;00m HTTPResponse\n\u001b[0;32m    460\u001b[0m \u001b[39m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[1;32m--> 461\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mgetresponse()\n\u001b[0;32m    463\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    464\u001b[0m     assert_header_parsing(httplib_response\u001b[39m.\u001b[39mmsg)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\http\\client.py:1374\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1372\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1373\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1374\u001b[0m         response\u001b[39m.\u001b[39;49mbegin()\n\u001b[0;32m   1375\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m:\n\u001b[0;32m   1376\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Python310\\lib\\http\\client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[39m# read until we get a non-100 response\u001b[39;00m\n\u001b[0;32m    317\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m--> 318\u001b[0m     version, status, reason \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_status()\n\u001b[0;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m status \u001b[39m!=\u001b[39m CONTINUE:\n\u001b[0;32m    320\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python310\\lib\\http\\client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_read_status\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m--> 279\u001b[0m     line \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mreadline(_MAXLINE \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m), \u001b[39m\"\u001b[39m\u001b[39miso-8859-1\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    280\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(line) \u001b[39m>\u001b[39m _MAXLINE:\n\u001b[0;32m    281\u001b[0m         \u001b[39mraise\u001b[39;00m LineTooLong(\u001b[39m\"\u001b[39m\u001b[39mstatus line\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    703\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m    704\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 705\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[0;32m    706\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[0;32m    707\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python310\\lib\\ssl.py:1274\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1270\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   1271\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1272\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[0;32m   1273\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[1;32m-> 1274\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[0;32m   1275\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1276\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\ssl.py:1130\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1128\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1129\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1130\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[0;32m   1131\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1132\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "get_summary(chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "returnresultforquestion(vectordbindex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " They take a few minutes to meditate and set their intentions for the day. They then make their bed and get dressed. After that, they make a cup of coffee and read for an hour. They then spend the rest of the day working, exercising, and spending time with family and friends. They also make sure to take breaks throughout the day and end their day with a few minutes of journaling and reflection.\n",
      " The speaker is also grateful for the people in their life who have helped them through difficult times.\n",
      "\n",
      " Finally, it is important to take a moment to appreciate the feeling of cleanliness and freshness that comes with brushing teeth. This can help to create a sense of gratitude and positivity for the day ahead.\n",
      " The narrator finds that this practice has helped them stay grounded and connected to their inner self, allowing them to navigate life with more ease and clarity.\n",
      " The book also provides guidance on how to integrate these practices into daily life.\n",
      "\n",
      "\n",
      "The goal of the meditation is to take a few moments to be mindful and connect to the divine energy within. The individual starts by sitting in a comfortable position and closing their eyes. They take a few deep breaths, allowing their body to relax. As they do this, they focus on their heart rate, breath rate, and skin temperature. They take note of any changes in these three areas and become aware of the subtle energy within their body.\n",
      "\n",
      "Once they have connected to this energy, they take a few moments to visualize a bright light emanating from their heart center, radiating outward and connecting them to the divine source. They visualize this light connecting them to a higher power, a source of unconditional love and joy. They take a few moments to bask in this energy, allowing it to fill them with a sense of peace and contentment.\n",
      "\n",
      "When they are ready, they open their eyes and take a few moments to reflect on the experience. They take note of any changes in their body and mind, and how they feel. They can then use this connection to the divine source to bring more love and joy into their daily life.\n",
      " It also allows them to reflect on their day and set an intention for the rest of it. Additionally, the Moments feature can be used to track and monitor moods over time, allowing users to identify patterns in their emotions.\n",
      "\n",
      "\n",
      "The key points in the context section include:\n",
      "\n",
      "1. Taking the time to reflect on the day and create the energy and mental space needed to live by higher values.\n",
      "\n",
      "2. Writing down goals for the day to help stay focused and motivated.\n",
      "\n",
      "3. Taking a moment to write down three things you are grateful for to help remind you to be the person you want to be.\n",
      "\n",
      "4. Practicing mindfulness and self-awareness to help stay in tune with your values and goals.\n",
      "\n",
      "\n",
      "When we start our day off with gratitude, we are setting an intention for the day. We are telling ourselves that we are thankful for what we have and that we are ready to take on the day with a positive attitude. This helps us to focus on the good in our lives and to be more mindful of our actions. It also helps us to be more mindful of how we interact with others. When we start our day off with gratitude, we are more likely to be kind, compassionate, and understanding.\n",
      "\n",
      "Gratitude also helps to put things into perspective. When we take the time to be grateful for what we have, we can better appreciate the small things in life. We can also recognize the challenges we face and be thankful for the lessons we learn from them.\n",
      "\n",
      "Overall, starting your day off with gratitude is a great way to set yourself up for success. It helps to create a positive mindset and to be more mindful of our actions. When we start our day off with gratitude, we are more likely to be kind, compassionate, and understanding. Gratitude also helps to put things into perspective and to appreciate the small things in life. So, take a few moments each morning to be grateful for what you have and start your day\n",
      " They also find it to be a great way to stay active and keep their mind sharp. Additionally, they enjoy the social aspect of playing tennis with friends or other players.\n",
      "\n",
      "\n",
      "The Aura Ring has been a great tool for tracking and verifying the impact of physical and mental health. It has allowed the person to track their activity, sleep, and heart rate, as well as their stress levels. This has enabled them to plan and improve their health, as well as to identify and address any potential health issues. The data collected by the Aura Ring has been very helpful in understanding the person’s overall health and wellbeing. Additionally, the app has made it easy to access and review the data collected by the Aura Ring.\n",
      " We can track our sleep and use this data to make more informed decisions about our sleep habits.\n",
      "\n",
      " They can now set goals for the day and measure their progress against them. This data also helps them to stay motivated and track their fitness journey.\n",
      " Additionally, it encourages the reader to find ways to stay active, such as taking a walk or doing yoga, and to make sure to get enough sleep. Finally, it emphasizes the importance of eating a balanced and nutritious diet.\n",
      " \n",
      "\n",
      "The section then goes on to discuss the benefits of chia seeds and how they are a great source of fiber, omega-3 fatty acids, protein, and antioxidants. It also mentions that chia seeds are a great way to add nutrition to your breakfast bowl. \n",
      "\n",
      "Finally, the section provides some tips on how to incorporate chia seeds into your breakfast bowl. It suggests adding them to smoothies, oatmeal, yogurt, and other breakfast items. It also suggests adding them to salads and other dishes for a nutritious boost.\n",
      " They then describe how they make the breakfast bowl, including the ingredients they use and how they prepare it. Finally, they mention how the breakfast bowl gives them energy and helps them stay focused throughout the day.\n",
      " They mention how they often take a few moments to appreciate the tea and how it helps them stay focused and balanced. The section ends with the narrator reflecting on the importance of having a guide in our pocket to help us make decisions and how herbal tea can be a great one.\n",
      " They are trying to learn from their mistakes and make sure that they are making progress towards the goals that they have set for themselves. They are also trying to be mindful of their habits and how they affect their life. They are trying to make sure that their habits are helping them become the best version of themselves.\n"
     ]
    }
   ],
   "source": [
    "for text in df['summary_chunks']:\n",
    "    print (text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def returnresultforquestion(vectordbindex):\n",
    "    questiontext = input(\"Enter your question: \")\n",
    "    question_embedding = get_embedding(text=questiontext, model=\"text-embedding-ada-002\")\n",
    "    content = \"\"\n",
    "    content = content\n",
    "    print (\"first\" + content)\n",
    "    vector_database_results_matching = vectordbindex.query([question_embedding], top_k=3, include_metadata=True, include_Values=True, \n",
    "        namespace=\"myyoutubeindex\")\n",
    "    for match in vector_database_results_matching['matches']:\n",
    "        if float(match['score']) *100 > 75 :\n",
    "            content = content + \" \" + match['metadata']['combined']\n",
    "    if content == \"\":\n",
    "        return \"Sorry, I could not find an answer to your question\"\n",
    "    \n",
    "    print (\"second\" + content)\n",
    "\n",
    "    # define the LLM you want to use\n",
    "    llm = OpenAI(temperature=0.4,openai_api_key=\"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\")\n",
    "\n",
    "    # define the context for the prompt by joining the most relevant text chunks\n",
    "    # context = \"\"\n",
    "\n",
    "    # for index, row in df[0:5].iterrows():\n",
    "    #     context = context + \" \" + row.text_chunks\n",
    "\n",
    "    # define the prompt template\n",
    "    template = \"\"\"\n",
    "    Answer the question in the question section stricty using only the contents from the context section only. If the context section is empty or If you cant find the answer, please respond that way\"\n",
    "\n",
    "    Context section:\n",
    "    {content}\n",
    "\n",
    "    Question section:\n",
    "    {users_question}\n",
    "\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = PromptTemplate(template=template, input_variables=[\"content\", \"users_question\"])\n",
    "\n",
    "    # fill the prompt template\n",
    "    prompt_text = prompt.format(content = content, users_question = questiontext)\n",
    "    answertext = llm(prompt_text )\n",
    "    return answertext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "def getcontextcontent()# create a list to store the calculated cosine similarity\n",
    "    # cos_sim = []\n",
    "\n",
    "    # for index, row in df.iterrows():\n",
    "    #    A = row.ada_embedding\n",
    "    #    B = question_embedding\n",
    "\n",
    "    #    # calculate the cosine similiarity\n",
    "    #    cosine = np.dot(A,B)/(norm(A)*norm(B))\n",
    "\n",
    "    #    cos_sim.append(cosine)\n",
    "\n",
    "    # df[\"cos_sim\"] = cos_sim\n",
    "    # df.sort_values(by=[\"cos_sim\"], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split transcript into chunks of size 2000\n",
    "def split_transcript_into_chunks(transcript, chunk_size):\n",
    "    chunks =[]\n",
    "    chunks = [transcript[i:i + chunk_size] for i in range(0, len(transcript), chunk_size)]\n",
    "    return chunks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text, model=\"text-embedding-ada-002\"):\n",
    "   text = text.replace(\"\\n\", \" \")\n",
    "   return openai.Embedding.create(input = [text], model=model)['data'][0]['embedding']\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
